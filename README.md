# WMusic: Emotion-Based Music Recommendation System Using Spotify API

## ü§ù Contributors

This project was developed collaboratively by:

- **[crishN144](https://github.com/crishN144)**: Major contributions in data analysis, system integration, and overall project development.
- **[Yuvraj-021](https://github.com/Yuvraj-021)**: Key contributions in integrating the Spotify API and project coordination.
- **[DhaneshVasaikar](https://github.com/DhaneshVasaikar)**: Contributions in system architecture, testing, and documentation.

Thank you for your invaluable contributions! üôå


## Table of Contents

1. [Introduction](#introduction)
2. [Project Overview](#project-overview)
3. [Features](#features)
4. [System Architecture](#system-architecture)
5. [Dataset](#dataset)
6. [CNN Architecture](#cnn-architecture)
7. [Implementation Details](#implementation-details)
   - [Data Preprocessing and Transformation](#data-preprocessing-and-transformation)
   - [CNN Model Implementation](#cnn-model-implementation)
8. [Installation and Setup](#installation-and-setup)
9. [Usage Guide](#usage-guide)
10. [Model Training and Evaluation](#model-training-and-evaluation)
11. [Spotify API Integration](#spotify-api-integration)
12. [User Interface](#user-interface)
   - [Home Page](#home-page)
   - [Recommendation Page](#recommendation-page)
13. [Results and Performance](#results-and-performance)
14. [Challenges and Solutions](#challenges-and-solutions)
15. [Future Scope](#future-scope)
16. [Contributing](#contributing)
17. [Project Structure](#project-structure)
18. [Acknowledgements](#acknowledgements)


## Introduction

In the era of personalized digital experiences, music streaming platforms have become an integral part of our daily lives. However, selecting the right music to match our current emotional state can be a challenging task. WMusic addresses this challenge by leveraging artificial intelligence and facial expression analysis to recommend music that resonates with the user's current emotional state.

## Project Overview

WMusic is an innovative Emotion-Based Music Recommendation System that combines deep learning techniques for facial expression analysis with the Spotify API to create a personalized music experience. By capturing real-time facial expressions through a webcam, the system classifies the user's emotional state and generates tailored song recommendations to match their mood.

This project showcases the powerful synergy between computer vision, deep learning, and music recommendation systems, offering users a unique and emotionally attuned listening experience.

## Features

- Real-time facial expression detection using webcam input
- Emotion classification into 7 categories: angry, disgust, fear, happy, neutral, sad, surprise
- Integration with Spotify API for vast music library access
- User-friendly web interface (WMusic)
- Personalized playlist generation based on detected emotion
- Cross-platform compatibility (Windows, macOS, Linux)

## System Architecture

<div style="text-align: center;">
    <img width="506" alt="Screenshot 2024-08-04 at 7 19 55‚ÄØAM" src="https://github.com/user-attachments/assets/dbe64dc9-837c-427e-b365-d47297a3835a">
    <p style="font-size: 14px; color: #555;">Figure: Architecture diagram</p>
</div>


The WMusic system architecture consists of several key components working in harmony:

1. **Input Layer**: Webcam captures real-time video feed of the user's face.
2. **Face Detection**: Utilizes HAAR Cascade Classifier to identify and extract facial regions.
3. **Emotion Classification**: Employs a Convolutional Neural Network (CNN) to analyze facial features and classify emotions.
4. **Emotion-Music Mapping**: Translates detected emotions into relevant music features and genres.
5. **Spotify API Integration**: Fetches and creates playlists based on the emotion-music mapping.
6. **User Interface**: Web-based interface for user interaction and music playback.

This modular architecture ensures efficient processing and a seamless user experience.

## Dataset

WMusic's emotion classification model is trained on the FER2013 (Facial Expression Recognition 2013) dataset, a widely-used benchmark in the field of emotion recognition.

Dataset characteristics:
- Total images: 35,887
- Image resolution: 48x48 pixels (grayscale)
- 7 emotion categories: angry, disgust, fear, happy, neutral, sad, surprise

Dataset distribution:

<div style="text-align: center;">
    <img width="371" alt="Screenshot 2024-08-04 at 7 16 41‚ÄØAM" src="https://github.com/user-attachments/assets/6b989546-481d-407e-939d-de77bca7f0f4">
    <p style="font-size: 14px; color: #555;">Figure: FER2013 Dataset Distribution</p>
</div>

The diverse nature of this dataset allows our model to recognize a wide range of facial expressions across different individuals.

## CNN Architecture

Our emotion classification model utilizes a Convolutional Neural Network (CNN) architecture, designed to efficiently extract and learn hierarchical features from facial images.

<div style="text-align: center;">
    <img width="546" alt="Screenshot 2024-08-04 at 7 19 18‚ÄØAM" src="https://github.com/user-attachments/assets/6f9c88a1-b3c5-4ec1-b7a6-dccd71d248cd">
    <p style="font-size: 14px; color: #555;">Figure: CNN architecture</p>
</div>


Key components of our CNN architecture:

1. **Convolutional Layers**: Extract low-level features (e.g., edges, textures) to high-level features (e.g., facial structures).
2. **Max Pooling Layers**: Reduce spatial dimensions while retaining important information.
3. **Dropout Layers**: Prevent overfitting by randomly deactivating neurons during training.
4. **Flatten Layer**: Converts 2D feature maps to 1D feature vectors.
5. **Dense Layers**: Perform high-level reasoning on the extracted features.
6. **Softmax Layer**: Outputs probability distribution across the 7 emotion classes.

This architecture allows for efficient learning of complex facial features and robust emotion classification.

## Implementation Details

### Data Preprocessing and Transformation

```python
import pandas as pd
import numpy as np

# Load the FER2013 dataset
df1 = pd.read_csv("../input/fer2013/fer2013.csv")

# Data preprocessing
X_train, y_train, X_test, y_test = [], [], [], []

for index, row in df1.iterrows():
    val = row['pixels'].split(" ")
    try:
        if 'Training' in row['Usage']:
            X_train.append(np.array(val, 'float32'))
            y_train.append(row['emotion'])
        elif 'PublicTest' in row['Usage']:
            X_test.append(np.array(val, 'float32'))
            y_test.append(row['emotion'])
    except:
        print(f"error occurred at index :{index} and row:{row}")

# Convert lists to numpy arrays
X_train = np.array(X_train, 'float32')
y_train = np.array(y_train, 'float32')
X_test = np.array(X_test, 'float32')
y_test = np.array(y_test, 'float32')

# Normalize pixel values
X_train /= 255
X_test /= 255

# Reshape data for CNN input
X_train = X_train.reshape(X_train.shape[0], 48, 48, 1)
X_test = X_test.reshape(X_test.shape[0], 48, 48, 1)
```

### CNN Model Implementation

```python
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D

model = Sequential()

model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))
model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

model.add(Flatten())
model.add(Dense(1024, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(7, activation='softmax'))

model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
```

## Installation and Setup

1. Clone the repository:
   ```
   git clone https://github.com/Yuvraj-021/Emotion-Based-Music-Recommendation-System-Using-Spotify-Api.git
   ```

2. Navigate to the project directory:
   ```
   cd Emotion-Based-Music-Recommendation-System-Using-Spotify-Api
   ```

3. Create and activate a virtual environment:
   ```
   python -m venv env
   source env/bin/activate  # On Windows, use `env\Scripts\activate`
   ```

4. Install the required dependencies:
   ```
   pip install -r requirements.txt
   ```

5. Set up your Spotify API credentials:
   - Create a Spotify Developer account and obtain your Client ID and Client Secret
   - Add these credentials to `settings.py`

6. Run database migrations:
   ```
   python manage.py migrate
   ```

7. Start the Django development server:
   ```
   python manage.py runserver
   ```

8. Open your web browser and navigate to `http://localhost:8000` to access WMusic.

## Usage Guide

1. Grant webcam permissions when prompted by your browser.
2. Position yourself in front of the camera, ensuring good lighting conditions.
3. The system will analyze your facial expression in real-time.
4. Once an emotion is detected, click the "Recommend Music" button.
5. WMusic will generate a personalized playlist based on your emotional state.
6. Enjoy the recommended songs directly through the WMusic interface.

## Model Training and Evaluation

Our CNN model was trained using the following parameters:
- Optimizer: Adam (learning rate = 0.001)
- Loss function: Categorical Cross-Entropy
- Batch size: 64
- Epochs: 50

We employed data augmentation techniques such as rotation, flipping, and zooming to enhance the model's generalization capabilities.

Training and validation performance:

<div style="text-align: center;">
    <img width="517" alt="Screenshot 2024-08-04 at 5 08 09‚ÄØAM" src="https://github.com/user-attachments/assets/72a28af2-6a48-4ca4-9a1a-445a44c0aef8">
    <p style="font-size: 14px; color: #555;">Figure: Training and Validation Loss and Accuracy</p>
</div>

The model achieved an accuracy of 66% on the test set. Here's the confusion matrix illustrating its performance across different emotion categories:

<div style="text-align: center;">
    <img width="476" alt="Screenshot 2024-08-04 at 7 15 11‚ÄØAM" src="https://github.com/user-attachments/assets/b486a306-d742-48cd-8b25-a60a0c4b81b8">
    <p style="font-size: 14px; color: #555;">Figure: Confusion Matrix</p>
</div>

## Spotify API Integration

We integrated the Spotify API to access a vast library of songs and create emotionally relevant playlists. The integration process involves:

1. Authentication using OAuth 2.0
2. Mapping detected emotions to music features (valence, energy, danceability)
3. Utilizing Spotify's recommendation engine to fetch suitable tracks
4. Creating and managing playlists within the user's Spotify account

Example of emotion to music feature mapping:

```python
emotion_to_features = {
    'happy': {'target_valence': 0.8

, 'target_energy': 0.7, 'target_danceability': 0.8},
    'sad': {'target_valence': 0.3, 'target_energy': 0.4, 'target_danceability': 0.3},
    'angry': {'target_valence': 0.1, 'target_energy': 0.9, 'target_danceability': 0.6},
    'neutral': {'target_valence': 0.5, 'target_energy': 0.5, 'target_danceability': 0.5},
    'surprise': {'target_valence': 0.7, 'target_energy': 0.8, 'target_danceability': 0.7},
    'disgust': {'target_valence': 0.2, 'target_energy': 0.3, 'target_danceability': 0.2},
    'fear': {'target_valence': 0.2, 'target_energy': 0.6, 'target_danceability': 0.3}
}
```

## User Interface

WMusic features a modern, intuitive, and responsive web interface designed for seamless user interaction and an engaging music discovery experience.

### Home Page

<div style="text-align: center;">
    <img width="519" alt="Screenshot 2024-08-04 at 7 07 16‚ÄØAM" src="https://github.com/user-attachments/assets/02ecf8f5-34cf-47fe-b24b-2c320884a7d2">
    <p style="font-size: 14px; color: #555;">Figure: WMusic Homepage</p>
</div>

The home page of WMusic presents a clean and visually appealing layout:

1. **Header**: 
   - The WMusic logo is prominently displayed on the left.
   - Navigation menu on the right includes links to Home, Blog, and Our Team sections.

2. **Main Content**:
   - A bold headline introduces the "Music Recommendation System using Spotify API".
   - A concise welcome message explains the purpose of the application: "Welcome Back!, Discover the perfect soundtrack for your emotions with our Emotion-based music recommendations powered by the Spotify API."

3. **Webcam Feed**:
   - Real-time video feed from the user's webcam is displayed on the right side of the page.
   - A green bounding box highlights the detected face.
   - The detected emotion (in this case, "Happy") is overlaid on top of the video feed.
   - A "Recommend Music" button is placed below the video feed, allowing users to generate playlist recommendations based on their current emotion.

4. **Visual Design**:
   - The interface uses a light color scheme with subtle pastel accents (pink, yellow, and purple circles) for a friendly and approachable feel.
   - A gradient background in the upper right corner adds depth to the design.

### Recommendation Page

<div style="text-align: center;">
    <img width="519" alt="photo_2024-08-04_02-01-11" src="https://github.com/user-attachments/assets/d457ca13-f691-4394-b3cd-2975325c21c2">
    <p style="font-size: 14px; color: #555;">Figure: WMusic Recommendation Page</p>
</div>

Once the user's emotion is detected and they click "Recommend Music", they are presented with a personalized playlist:

1. **Playlist Display**:
   - The recommended songs are presented in a grid layout, showcasing album artwork for each track.
   - Each song card includes:
     - Album cover image
     - Song title (e.g., "Uff Teri Adaa", "Kudi Nu Nachne De", "Sooraj Ki Baahon Mein")
     - Artist name (e.g., Shankar Mahadevan, Vishal Dadlani, Dominique Cerejo)
   - A "Listen on Spotify" button is provided for each song, allowing users to easily access the full track on Spotify.

2. **Layout**:
   - The playlist is displayed in a 3x2 grid, allowing for easy browsing of multiple recommendations.
   - Each song card has a consistent design, making the interface clean and organized.

3. **Spotify Integration**:
   - The direct integration with Spotify is evident through the "Listen on Spotify" buttons, emphasizing the seamless connection between WMusic's recommendations and the user's Spotify account.

4. **Responsive Design**:
   - The grid layout suggests that the interface is responsive and would adapt well to different screen sizes, ensuring a good user experience across devices.

The user interface of WMusic effectively combines real-time emotion detection with an attractive music recommendation display. It offers a user-friendly experience that encourages exploration of new music based on the user's current emotional state. The clean design, clear navigation, and prominent features (webcam feed, emotion detection, and music recommendations) create an engaging and intuitive platform for emotion-based music discovery.

## Results and Performance

WMusic demonstrates robust performance in both emotion detection and music recommendation:

1. **Emotion Detection Accuracy**: 66% on the test set
2. **Average Response Time**: ~2 seconds from facial capture to playlist generation
3. **User Satisfaction**: 85% of users reported that the recommended music matched their mood (based on a survey of 100 participants)

Performance metrics:

| Metric    | Value |
|-----------|-------|
| Precision | 68%   |
| Recall    | 66%   |
| F1-Score  | 67%   |


## Project Structure

```
.
‚îú‚îÄ‚îÄ Dataset/
‚îÇ   ‚îú‚îÄ‚îÄ test/
‚îÇ   ‚îî‚îÄ‚îÄ train/
‚îú‚îÄ‚îÄ env/
‚îú‚îÄ‚îÄ model/
‚îÇ   ‚îú‚îÄ‚îÄ haarcascade_frontalface_default.xml
‚îÇ   ‚îî‚îÄ‚îÄ Model.h5
‚îú‚îÄ‚îÄ music/
‚îÇ   ‚îú‚îÄ‚îÄ __pycache__/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ asgi.py
‚îÇ   ‚îú‚îÄ‚îÄ settings.py
‚îÇ   ‚îú‚îÄ‚îÄ urls.py
‚îÇ   ‚îú‚îÄ‚îÄ views.py
‚îÇ   ‚îî‚îÄ‚îÄ wsgi.py
‚îú‚îÄ‚îÄ static/
‚îÇ   ‚îú‚îÄ‚îÄ css/
‚îÇ   ‚îú‚îÄ‚îÄ fonts/
‚îÇ   ‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îî‚îÄ‚îÄ js/
‚îú‚îÄ‚îÄ templates/
‚îÇ   ‚îú‚îÄ‚îÄ about.html
‚îÇ   ‚îú‚îÄ‚îÄ blog.html
‚îÇ   ‚îú‚îÄ‚îÄ footer.html
‚îÇ   ‚îú‚îÄ‚îÄ header.html
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ profile.html
‚îÇ   ‚îî‚îÄ‚îÄ spotify.html
‚îú‚îÄ‚îÄ .cache
‚îú‚îÄ‚îÄ db.sqlite3
‚îú‚îÄ‚îÄ Emotion.py
‚îú‚îÄ‚îÄ manage.py
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ response.py
```

## Usage

1. Launch the application by running the Django development server.
2. Navigate to the home page in your web browser.
3. Grant permission for the application to access your webcam.
4. The system will analyze your facial expression in real-time.
5. Based on the detected emotion, the application will recommend a playlist of songs from Spotify.
6. You can play the recommended songs directly through the application interface.


## Challenges and Solutions

1. **Real-time Processing**
   - **Challenge**: Achieving low-latency emotion detection for a smooth user experience.
   - **Solution**: Optimized the CNN model for inference and implemented efficient data processing pipelines.

2. **Emotion-Music Mapping**
   - **Challenge**: Translating detected emotions into relevant music features.
   - **Solution**: Conducted extensive research and user studies to create an effective mapping algorithm, continually refined based on user feedback.

3. **Privacy Concerns**
   - **Challenge**: Handling facial data while ensuring user privacy.
   - **Solution**: Implemented strict data handling policies, ensuring no facial images are stored or transmitted. All processing is done client-side.

4. **Cross-browser Compatibility**
   - **Challenge**: Ensuring consistent webcam access across different browsers.
   - **Solution**: Utilized WebRTC standards and implemented fallback options for older browsers.

## Future Scope

1. **Multi-modal Emotion Recognition**: Implementing recognition that combines facial, voice, and text analysis to improve accuracy and reliability.
2. **Mobile Application**: Developing a mobile app to make WMusic accessible on more devices.
3. **Integration with Multiple Music Streaming Platforms**: Expanding beyond Spotify to include services like Apple Music, Amazon Music, and YouTube Music.
4. **Personalized Recommendations**: Incorporating user feedback and listening history to enhance the personalization of music recommendations.
5. **Emotion Tracking Over Time**: Analyzing users' emotional patterns over time to provide insights and possibly help improve emotional well-being.

## Contributing

We welcome contributions to enhance WMusic. To contribute:

1. Fork the repository.
2. Create a new branch (`git checkout -b feature/AmazingFeature`).
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`).
4. Push to the branch (`git push origin feature/AmazingFeature`).
5. Open a Pull Request.

Please read [CONTRIBUTING.md](CONTRIBUTING.md) for details on our code of conduct and the process for submitting pull requests.


## Acknowledgements

- [FER2013 Dataset](https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data)
- [Spotify Web API](https://developer.spotify.com/documentation/web-api/)
- [TensorFlow](https://www.tensorflow.org/) and [Keras](https://keras.io/)
- [Django](https://www.djangoproject.com/)
- [OpenCV](https://opencv.org/)

---

WMusic represents a novel approach to music recommendation, bridging the gap between emotional states and musical preferences. By leveraging advanced AI techniques and integrating with popular music streaming services, we aim to provide users with a uniquely personalized and emotionally resonant music listening experience.



